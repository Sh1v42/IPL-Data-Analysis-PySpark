# 📊 IPL Data Analysis Using PySpark, Databricks, and AWS S3

## 📚 Overview

This project explores and analyzes Indian Premier League (IPL) cricket data using PySpark on Databricks.
The datasets are sourced from AWS S3 storage and processed using distributed computing techniques for scalability and performance.

The objective is to uncover player performance trends, team statistics, and match insights through large-scale data processing and analysis.


## ⚙️ Technologies Used

1. PySpark

2. Databricks

3. AWS S3

4. Python 3.8+

5. Apache Spark SQL

## 🧩 Dataset Information

The following datasets were used (stored on AWS S3):

- Ball_By_Ball.csv

- Match.csv

- Player.csv

- Player_Match.csv

- Team.csv

Each dataset covers different aspects of IPL matches, including ball-by-ball actions, match summaries, team compositions, and player statistics.

## 🧪 Project Workflow

### Spark Session Setup: 
  
-  Initialize a PySpark session inside Databricks.

-  Schema Definition: Define structured schemas for each dataset.

-  Data Loading: Read datasets directly from AWS S3 into Spark DataFrames.

### Data Preprocessing:

-   Handle missing values

-   Perform data type corrections

-   Apply basic transformations

-   Exploratory Data Analysis (EDA):

-   Analyze player performances

-   Study team winning trends

-  Identify top performers (batsmen, bowlers)

### Visualizations 

## 🔗 DataBricks Pulished Link 

    https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1995427356338600/4132950691476681/7330905137864293/latest.html

        
